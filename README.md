<div align="center">

<!-- You can replace this with your own logo -->
<img src="https://placehold.co/150x150/2d3748/ffffff?text=CHEM-AD&font=inter" alt="Project Logo">

# Unsupervised Anomaly Detection using an Autoencoder

A deep learning approach to identify outliers in complex datasets without pre-labeled data.

<!-- Badges: Replace with your own links -->
![Python 3.9+](https://img.shields.io/badge/Python-3.9+-blue.svg)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange.svg)
![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)

**[Link to the Research Paper]** 

</div>

---

### üìÑ **Abstract & Project Goal**

This repository contains the complete code and workflow for a research paper on **unsupervised anomaly detection**. The primary objective is to identify unusual patterns or outliers in a dataset without prior knowledge or labeled examples of what constitutes an anomaly.

The core of this project is an **autoencoder**, a type of neural network trained to reconstruct its input data. The model is trained exclusively on normal data instances. When presented with a new data point, the model's ability to reconstruct it is measured; a high **reconstruction error** indicates that the data point is significantly different from the training data and is therefore flagged as an anomaly. The subsequent analysis involves using dimensionality reduction techniques and various plotting methods to visualize and interpret the characteristics of the detected anomalies.

---

### üöÄ **Workflow Pipeline**

The project follows a clear, sequential workflow. The notebooks are designed to be run in order, as the outputs of the initial modeling steps are the inputs for the later analysis and visualization steps.

**`Data Input`** ‚û°Ô∏è **`1. Anomaly.ipynb`** (Model Training & Scoring) ‚û°Ô∏è **`Generated CSV Outputs`** ‚û°Ô∏è **`2. Analysis Notebooks`** (Visualization) ‚û°Ô∏è **`Final Figures`**

---

### üìÇ **File Descriptions & Purpose**

This project is organized into several Jupyter notebooks, each with a specific role in the pipeline.

#### **Core Model File**

* `Anomaly.ipynb`
    * **Purpose**: This is the main and most critical notebook. It handles the entire modeling process from start to finish.
    * **Key Functions**:
        1.  **Data Loading & Preprocessing**: Loads the raw dataset (`dataset.csv`), cleans it, and scales the features for the neural network.
        2.  **Autoencoder Model**: Defines, compiles, and trains the autoencoder model on the processed data.
        3.  **Anomaly Score Calculation**: Uses the trained model to predict on the dataset and calculates the reconstruction error for each data point.
        4.  **Output Generation**: Saves the data with their corresponding anomaly scores into CSV files (`anomaly.csv`, `anomaly_all.csv`) in the `outputs/` folder. **This notebook must be run first.**
            * `anomaly.csv`: Contains only the *averaged* anomaly scores per data point.
            * `anomaly_all.csv`: Contains the detailed anomaly scores *across all features*.

#### **Post-Modeling & Visualization Files**

* `Dimention Reduction.ipynb`
    * **Purpose**: To visualize the high-dimensional dataset in a 2D space to see if the detected anomalies form distinct clusters.
    * **Methods**: Utilizes powerful dimensionality reduction techniques like **UMAP** or **t-SNE**.
    * **Input**: Reads the `anomaly_all.csv` file generated by `Anomaly.ipynb`.

* `Plots 1.ipynb` & `Plots 2.ipynb`
    * **Purpose**: These notebooks are dedicated to creating the specific, publication-quality figures and charts required for the research paper. They also include analysis using the **Elbow Method** to determine an optimal threshold for classifying anomalies.
    * **Key Functions**: Generate various visualizations such as violin plots, scatter plots, and feature distributions to compare normal vs. anomalous data points.
    * **Output**: Saves the generated figures into the `Figs/` directory.
    * **Input**: Reads the `anomaly_all.csv` file.

---

### ‚öôÔ∏è **How to Run the Project**

To reproduce the results, follow these steps in order:

1.  **Clone the Repository**
    ```bash
    git clone https://github.com/alimardani76/Anomaly_Detection_CHEM_AD
    cd Anomaly_Detection_CHEM_AD
    ```

2.  **Install Dependencies**
    Ensure you have the required libraries installed.
    ```bash
    pip install pandas numpy tensorflow scikit-learn matplotlib seaborn umap-learn jupyter
    ```

3.  **Run the Main Model**:
    * Open and run all cells in `Anomaly.ipynb`. This will train the model and create the necessary `anomaly_all.csv` file in the `outputs` folder.

4.  **Run the Analysis & Plotting Notebooks**:
    * Once Step 1 is complete, you can run `Dimention Reduction.ipynb`, `Plots 1.ipynb`, and `Plots 2.ipynb` in any order to generate the figures for the paper.
  
# Feature Extraction Scripts for Anomaly Detection

This folder contains the Python scripts used for the initial feature extraction, which is a necessary pre-processing step for the main anomaly detection analysis. These scripts process raw Metal-Organic Framework (MOF) structural files (`.cif` and `.json`) to generate a comprehensive set of geometric, chemical, and topological features.

The final output is a `.csv` file, which serves as the input dataset for the anomaly detection model. The pipeline is designed to work with large datasets like the **CoRE MOF 2019** and **hMOF** databases.

## üî¨ Features Extracted

This pipeline extracts over 50 features, categorized as follows:

* **Geometric:** Surface area, void fraction, pore limiting diameter (PLD), and largest cavity diameter (LCD).
* **Chemical:** Density, formula, elemental properties (e.g., average electronegativity), metal fractions, and one-hot encoding of common metals.
* **Topological:** Graph-based properties describing the MOF's connectivity, such as graph density, diameter, average shortest path length, and clustering coefficients.
* **Linker & Metal:** Properties specific to the organic linkers and metal centers, such as average bond lengths and metal coordination numbers.

## üöÄ Workflow & How to Use

The data processing is broken down into a sequence of scripts. You should run them in order, as each script's output may be the input for the next one.

### **Prerequisites**

Make sure you have the required Python libraries installed:

```
pip install pandas pymatgen networkx tqdm
```

You will also need to have your raw MOF dataset folders (e.g., `CoREMOF 2019`, `hMOF-10_CO2_CH4_N2`) in the same directory as these scripts.

### **Step-by-Step Instructions**

1.  **Prepare the Dataset (`01_prepare_dataset.py`)**
    * **What it does:** Finds all `.cif` files in your raw dataset folders, matches them with their corresponding `.json` files, and copies them into a clean project directory (`MOFxDB_Project`).
    * **How to run:**
        ```
        python 01_prepare_dataset.py
        ```

2.  **Extract Geometric Features (`02_extract_geometric_features.py`)**
    * **What it does:** Reads the `.json` files to extract pre-calculated geometric properties.
    * **How to run:**
        ```
        python 02_extract_geometric_features.py
        ```

3.  **Extract Chemical Features (`03_extract_chemical_features.py`)**
    * **What it does:** Analyzes the `.cif` files to calculate a wide range of chemical and compositional properties. This script processes the full CoRE MOF set and a 15k subset of hMOFs.
    * **How to run:**
        ```
        python 03_extract_chemical_features.py
        ```

4.  **Extract Topological Features (`04_extract_topological_features.py`)**
    * **What it does:** Treats each MOF as a mathematical graph to calculate features describing its connectivity and topology.
    * **How to run:**
        ```
        python 04_extract_topological_features.py
        ```

5.  **Extract Linker & Metal Features (`05_extract_linker_metal_features.py`)**
    * **What it does:** Calculates features specific to the organic linkers and metal nodes within the MOF structures.
    * **How to run:**
        ```
        python 05_extract_linker_metal_features.py
        ```

After running all the scripts, your extracted features will be located in the `MOFxDB_Project/features/` directory, ready for your analysis!

